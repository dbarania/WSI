{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,sizes:list) -> None:\n",
    "        self._num_layers = len(sizes)-1\n",
    "        self._sizes = sizes\n",
    "        self._weights, self._bias  = self._initialize_weights()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(dataset_file_handle:h5py.File):\n",
    "        for i,key in enumerate(dataset_file_handle.keys()):\n",
    "            match i:\n",
    "                case 0:\n",
    "                    list_of_classes = np.array(dataset_file_handle[key][:])\n",
    "                case 1:\n",
    "                    x_data = np.array(dataset_file_handle[key][:])\n",
    "                case 2:\n",
    "                    y_data = np.array(dataset_file_handle[key][:])\n",
    "                            \n",
    "        return list_of_classes,x_data.reshape(x_data.shape[0],-1),y_data\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z:np.ndarray):\n",
    "        return np.exp(Z)/(1+np.exp(Z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(Z:np.ndarray):\n",
    "        return NeuralNetwork.sigmoid(Z)*(1-NeuralNetwork.sigmoid(Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(Z:np.ndarray):\n",
    "        return (Z+np.abs(Z))/2\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(y_pred:np.ndarray,y_data:np.ndarray):\n",
    "        return np.mean(np.power((y_data-y_pred),2))    \n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_derivative(y_pred:np.ndarray,y_data:np.ndarray):\n",
    "        return 2*(y_pred-y_data)/y_data.size\n",
    "\n",
    "    def _initialize_weights(self)->list[np.ndarray]:\n",
    "        sizes = self._sizes\n",
    "        weights_list = list()\n",
    "        bias_list = list()\n",
    "        for i in range(self._num_layers):\n",
    "            temp = np.sqrt(sizes[i])\n",
    "            weights_list.append(np.random.uniform(-1/temp,1/temp,(sizes[i+1],sizes[i])))\n",
    "            bias_list.append(np.random.uniform(-1,1,sizes[i+1]))\n",
    "        return weights_list, bias_list\n",
    "  \n",
    "    def forward_propagation(self, X:np.ndarray):\n",
    "        layers_output_list = list()\n",
    "        A = X\n",
    "        layers_output_list.append(A)\n",
    "        for i in range(self._num_layers):\n",
    "            # Z = self._weights[i].dot(A)\n",
    "            # Y = self.sigmoid(Z)\n",
    "            # A = np.append(Y,1)\n",
    "            Z = np.dot(self._weights[i],A)+self._bias[i]\n",
    "            Y = self.sigmoid(Z)\n",
    "            A = Y\n",
    "            layers_output_list.append(A)\n",
    "        # layers_output_list[-1] = np.delete(layers_output_list[-1],-1,0)\n",
    "        return layers_output_list\n",
    "    \n",
    "    def backward_propagation(self, layers_output_list:list[np.ndarray], Y:np.ndarray):\n",
    "        y_pred = layers_output_list[-1]\n",
    "        dcdy = self.mse_derivative(y_pred,Y)\n",
    "        dydz = self.sigmoid_derivative(y_pred)\n",
    "        dw = [0]*self._num_layers\n",
    "        db = [0]*self._num_layers\n",
    "\n",
    "\n",
    "        for i in reversed(range(self._num_layers)):\n",
    "            # dCdw = np.multiply(dCdY,dydz_old).dot(layers_output_list[i])\n",
    "                # dCdw = np.multiply(dCdY,dydz_old).dot(layers_output_list[i]) if np.multiply(dCdY,dydz_old).shape!=np.zeros((1)).shape \\\n",
    "                #     else np.multiply(np.multiply(dCdY,dydz_old),(layers_output_list[i]))\n",
    "                # dCdY = self._weights[i].transpose().dot(np.multiply(dCdY,dydz_old))\n",
    "            # dCdw = np.outer(np.multiply(dCdY.transpose()[0],dydz_old),layers_output_list[i])\n",
    "            # dCdX = np.multiply(self._weights[i], np.multiply(dCdY.transpose(),dydz_old))\n",
    "            # dCdY = np.delete(dCdX,-1)\n",
    "            dcdw = np.outer((dcdy*dydz),layers_output_list[i].T)\n",
    "            dcdb = dcdy\n",
    "            dcdx = np.dot(self._weights[i].T,dcdy*dydz)\n",
    "            dcdy = dcdx\n",
    "            dydz = self.sigmoid_derivative(layers_output_list[i]).transpose()\n",
    "            dw[i] = dcdw\n",
    "            db[i] = dcdb\n",
    "        return dw, db\n",
    "       \n",
    "    def update_parameters(self,learing_late,grad_dw,grad_db):\n",
    "        for i in range(self._num_layers):\n",
    "            self._weights[i]-=learing_late*grad_dw[i]\n",
    "            self._bias[i]-=learing_late*grad_db[i]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sum_list(l1:list[np.ndarray],l2:list[np.ndarray]):\n",
    "        result = list()\n",
    "        for el1,el2 in zip(l1,l2):\n",
    "            result.append(el1+el2)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def _scale_ipnut(X:np.ndarray):\n",
    "        return X/255\n",
    "    \n",
    "    def train_model(self, train_data_file_handle:h5py.File, iterations:int, learing_rate:float):\n",
    "        classes, train_data_x,train_data_y = self.load_data(train_data_file_handle)\n",
    "        for _ in range(iterations):\n",
    "            grad_dw_sum = None\n",
    "            grad_db_sum = None\n",
    "\n",
    "            for X,Y in zip(train_data_x, train_data_y):\n",
    "                X = self._scale_ipnut(X)\n",
    "                A = self.forward_propagation(X)\n",
    "                dw,db = self.backward_propagation(A,Y)\n",
    "                if grad_dw_sum and grad_db_sum:\n",
    "                    self._sum_list(grad_dw_sum,dw)\n",
    "                    self._sum_list(grad_db_sum,db)\n",
    "                else:\n",
    "                    grad_dw_sum = dw\n",
    "                    grad_db_sum = db\n",
    "\n",
    "            grad_dw = [el/train_data_y.size for el in grad_dw_sum]\n",
    "            grad_db = [el/train_data_y.size for el in grad_db_sum]\n",
    "            self.update_parameters(learing_rate, grad_dw, grad_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n",
      "C:\\Users\\damia\\AppData\\Local\\Temp\\ipykernel_10384\\574622620.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(Z)/(1+np.exp(Z))\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"data/train_catvnoncat.h5\") as train_file:\n",
    "    network = NeuralNetwork([64*64*3,10,1])\n",
    "    network.train_model(train_file,10,0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
